import { BlogPost } from '../types';

// This file is auto-generated by scripts/generate_blog_posts.py
export const BLOG_POSTS: BlogPost[] = [
  {
    "id": "2025-12-08-me-and-my-gpt-why-i-stopped-letting-chatbots-do-my-statistics",
    "title": "Me and my GPT: Why I Stopped Letting Chatbots Do My Statistics",
    "date": "2025-12-08",
    "summary": "Don't ask GPT for a p-value. Ask it to help you write the code, explain the method, and keep you in control.",
    "content": [
      "Increasingly, I hear some version of the same story: \"I pasted the numbers into ChatGPT, and it gave me an odds ratio and p-value, is that OK?\" My short answer is probably not. But there is a much better way to bring AIs into your analysis workflow, one that keeps you in control, improves transparency, and might even teach you some statistics along the way.",
      "My own turning point came with a small clinical dataset from an observational study. I tried the \"lazy\" approach first, summarised the data in a paragraph, pasted some counts into GPT and asked for an odds ratio. Within seconds, I had a beautifully formatted answer. But I had no idea which test had been used, whether any continuity corrections had been applied, or whether GPT had quietly misread my numbers. When I later ran the analysis myself in R, the result was close but not identical.",
      "That was the moment I decided: I don't want the chatbot to be my statistician. I want it to be my coding tutor and help me write those tests that I will struggle to put into formal language.",
      "For data analysis, I follow a simple rule: never ask GPT for a result. Ask it to help you inspect, write code, suggest analysis and explain what it's doing. This shifts GPT into three roles at once: explainer, code generator, and thinking partner. I still have to run the code myself and look at the outputs. But now I can see the entire pipeline from raw data to conclusion. The black box has been opened.",
      "LLMs make it trivially easy to try dozens of models. This is powerful and dangerous in equal measure. AI can act as a p-hacking accelerator if we're not careful. Some simple guardrails help: start with the question, not the model. Write down your primary outcome and primary analysis before you begin. Keep a record of your prompts and scripts as part of your analysis log.",
      "GPT can be an excellent assistant at the data desk, but the responsibility for the analysis, and the patients and policies it will ultimately affect, still rests firmly with us."
    ],
    "tags": [
      "LLMs",
      "Statistics",
      "Research Methods"
    ],
    "externalLink": "https://aihealthalliance.org/2025/12/08/me-and-my-gpt-why-i-stopped-letting-chatbots-do-my-statistics/"
  },
  {
    "id": "2025-09-29-me-and-my-gpt-from-fake-citations-to-a-research-assistant",
    "title": "Me and my GPT: From Fake Citations to a Research Assistant",
    "date": "2025-09-29",
    "summary": "GPT has gone from confidently fabricating references to competently retrieving, integrating, and checking them.",
    "content": [
      "Last month, while writing a grant, I found myself stuck. Three half-finished paragraphs sat waiting for references. Normally, this would mean an hour trawling PubMed, copy-pasting titles, and staring at a blank page trying to integrate them. Instead, I pasted one citation I had already collected into ChatGPT and asked it to draft a paragraph placing the study in context. The result wasn't perfect, but it gave me a solid first draft and broke my writer's block.",
      "That was my \"aha\" moment: GPT wasn't replacing my work, but it was shifting it. Less clerical time, more critical editing.",
      "When ChatGPT first appeared in late 2022, it seemed like a potential shortcut for references. The reality was less helpful: the model confidently produced lists of references, but many were fabricated. Titles and journals looked convincing, but the articles didn't exist. In healthcare research, that's not just an error, it undermines trust.",
      "Fast forward to 2025. The situation is changing. GPT can now connect to real databases, search the web, retrieve verifiable citations, and even support more advanced workflows. I now use it for three things: retrieving references without hallucination, integrating them into writing, and verifying citations during peer review.",
      "As a peer reviewer, I often need to check whether references truly support an author's claim. Instead of pulling the paper manually, I can paste the claim and the citation into GPT and ask: \"Summarise what this paper reports. Does it support the claim?\" In one case, it flagged that the cited paper only showed moderate agreement with human reviewers, not superiority.",
      "If GPT once symbolised unreliability through fake citations, it now symbolises a new challenge: how do we responsibly integrate a tool that is finally beginning to get the references right? The transformation is not about replacing researchers. It is about changing where we spend our energy: away from mechanical tasks, toward oversight and interpretation."
    ],
    "tags": [
      "LLMs",
      "Research",
      "Academic Writing"
    ],
    "externalLink": "https://aihealthalliance.org/2025/09/29/me-and-my-gpt-from-fake-citations-to-a-research-assistant/"
  }
];
